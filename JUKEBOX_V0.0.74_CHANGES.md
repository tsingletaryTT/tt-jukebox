# TT-Jukebox Full Automation (v0.0.74)

## Summary

Completely automated TT-Jukebox to handle **everything** - from cloning repos to downloading models - with direct Python execution. No more bash scripts, no more manual steps. Just run jukebox, confirm, and get your explicit vLLM command.

## User Feedback

**Error reported:** `python setup-scripts/setup_llama-3_1-8b.sh` gave syntax error
- User was trying to run bash script with `python` instead of `bash`

**Core request:** "I still want it more automated. Make install of metal and vllm invoked by tt-jukebox itself."

**Goal:** Maximum automation - jukebox should handle everything automatically, only show explicit vLLM command at the end.

## What Changed

### Full Automation Added

**1. Auto-install tt-metal and vLLM if missing:**
- `install_tt_metal()` - Clones tt-metal from GitHub if not found
- `install_tt_vllm()` - Clones vLLM from GitHub if not found
- `detect_tt_metal()` - Now calls `install_tt_metal()` if not found
- `detect_tt_vllm()` - Now calls `install_tt_vllm()` if not found

**2. Direct Python execution (no bash scripts):**
- `execute_setup()` - Replaces `generate_setup_script()`
- Directly executes: git checkouts, builds, model downloads
- Uses Python `subprocess.run()` for all operations
- Real-time feedback with colored output
- Proper error handling and timeouts

**3. Simplified workflow:**
```bash
# Old v0.0.73 (generate script, then run it)
python3 tt-jukebox.py --model llama --setup
bash ~/tt-scratchpad/setup-scripts/setup_llama_3_1_8b_instruct.sh

# New v0.0.74 (automatic execution)
python3 tt-jukebox.py --model llama
# Confirms, then automatically runs setup
# Shows explicit vLLM command when done
```

## New Workflow

### Example: First Time User

```bash
$ python3 tt-jukebox.py --model llama

======================================================================
TT-JUKEBOX: Model & Environment Manager
======================================================================

Current Environment
â„¹ Checking tt-metal installation...
âš  tt-metal not found - will install automatically

ðŸ“¦ Installing tt-metal...
â„¹ Cloning tt-metal to /home/user/tt-metal...
âœ“ tt-metal cloned

â„¹ Checking tt-vllm installation...
âš  tt-vllm not found - will install automatically

ðŸ“¦ Installing tt-vllm...
â„¹ Cloning tt-vllm to /home/user/tt-vllm...
âœ“ tt-vllm cloned

[... hardware detection, model specs fetch ...]

Matching Configurations

[1] Llama-3.1-8B-Instruct
    Device: N150
    tt-metal commit: 9b67e09
    vLLM commit: a91b644
    Model: Not downloaded

Select a configuration (1-1) or 'q' to quit: 1

ðŸš€ Starting automated setup...
âš  This will:
  â€¢ Checkout correct tt-metal and vLLM commits
  â€¢ Rebuild tt-metal if commit changed
  â€¢ Download model from HuggingFace if missing

Continue? [Y/n]: y

======================================================================
Setting up environment for Llama-3.1-8B-Instruct
======================================================================

ðŸ“¦ Checking out tt-metal...
âœ“ tt-metal checked out to 9b67e09

ðŸ”¨ Building tt-metal (this may take 5-10 minutes)...
âœ“ tt-metal built successfully

ðŸ“¦ Checking out tt-vllm...
âœ“ tt-vllm checked out to a91b644

ðŸ“¥ Downloading model from HuggingFace...
âœ“ Model downloaded successfully

======================================================================
âœ… Setup Complete!
======================================================================

Environment configured for Llama-3.1-8B-Instruct
  tt-metal: 9b67e09
  tt-vllm: a91b644
  Model: /home/user/models/Llama-3.1-8B-Instruct

ðŸ’¡ You can now run the vLLM server command shown below

======================================================================
Ready-to-Run vLLM Command
======================================================================

Start vLLM Server:
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && export MESH_DEVICE=N150 && export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 65536 \
    --max-num-seqs 16 \
    --block-size 64

Test Server:
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "max_tokens": 128
  }'

âœ“ Ready to run! Copy-paste the vLLM command above to start the server
```

### Example: Environment Already Configured

```bash
$ python3 tt-jukebox.py --model llama

[... detection shows everything matches ...]

âœ“ tt-metal already on correct commit (9b67e09)
âœ“ tt-vllm already on correct commit (a91b644)
âœ“ Model already downloaded at /home/user/models/Llama-3.1-8B-Instruct

[... shows vLLM command directly ...]

âœ“ Ready to run! Copy-paste the vLLM command above to start the server
```

## Code Changes

### File: `content/templates/tt-jukebox.py`

**Added Functions:**

1. **`install_tt_metal()` (lines 154-205)**
   - Clones tt-metal from GitHub if not found
   - Returns info dict with path, commit, branch
   - 5 minute timeout for clone operation

2. **`install_tt_vllm()` (lines 277-327)**
   - Clones vLLM from GitHub if not found
   - Returns info dict with path, commit, branch
   - 5 minute timeout for clone operation

**Modified Functions:**

3. **`detect_tt_metal()` (lines 208-274)**
   - Now calls `install_tt_metal()` if not found
   - Changed message: "tt-metal not found - will install automatically"

4. **`detect_tt_vllm()` (lines 330-382)**
   - Now calls `install_tt_vllm()` if not found
   - Changed message: "tt-vllm not found - will install automatically"

5. **`execute_setup()` (lines 869-1033)** - **COMPLETE REWRITE**
   - Was: `generate_setup_script()` - returned bash script string
   - Now: `execute_setup()` - directly executes all setup steps
   - Returns: `bool` (True if successful, False if failed)

   **Operations performed:**
   - Git checkout tt-metal to specific commit
   - Build tt-metal if commit changed (15 min timeout)
   - Git checkout vLLM to specific commit
   - Download model from HuggingFace (30 min timeout)
   - All with proper error handling and colored output

6. **`display_cli_commands()` (lines 1118-1128)**
   - Removed `setup_script_path` parameter
   - Simplified to show only vLLM command and test command
   - No more step numbering (setup is automatic)

7. **`main()` (lines 1398-1430)**
   - Removed bash script generation code
   - Removed `--setup` flag logic
   - Now **automatically executes setup** if needed
   - Asks for confirmation (unless `--force` flag)
   - Shows vLLM command after setup completes

**Removed:**
- `--setup` flag (setup is now automatic)
- `~/tt-scratchpad/setup-scripts/` directory usage
- All bash script generation code
- Intermediate script files

## Key Benefits

âœ… **Zero manual steps:** Everything automated from clone to checkout to build to download

âœ… **No bash script confusion:** Direct Python execution, no intermediate files

âœ… **Idempotent:** Safe to run multiple times, only does what's needed

âœ… **Auto-install:** Clones tt-metal and vLLM if not found

âœ… **Clear feedback:** Real-time colored output shows progress

âœ… **Proper error handling:** Timeouts, error messages, return codes

âœ… **Still explicit vLLM command:** Final command is copy-pasteable and transparent

âœ… **Confirmation prompt:** User can review what will happen before proceeding

## Command Line Options

```bash
# Basic usage (auto-setup)
python3 tt-jukebox.py --model llama

# Skip confirmation prompt
python3 tt-jukebox.py --model llama --force

# List available models
python3 tt-jukebox.py --list

# Include experimental models
python3 tt-jukebox.py --model mistral --show-experimental

# Set HF token for gated models
python3 tt-jukebox.py --model llama --hf-token hf_...
```

## Error Handling

**Proper timeouts:**
- Git operations: 30-60 seconds
- tt-metal build: 15 minutes
- Model download: 30 minutes

**Clear error messages:**
- Failed git checkout: Shows stderr output
- Build failure: Shows build errors
- HF auth failure: Explains how to authenticate
- Timeout: Shows which operation timed out

**Return codes:**
- `0` = Success
- `1` = Setup failed
- `130` = User interrupted (Ctrl+C)

## Testing

- âœ… Syntax validated: `python3 -m py_compile` passes
- âœ… Build succeeded: `npm run build`
- âœ… Extension packaged: **v0.0.74** (487.85 KB, 105 files)

## Comparison with Previous Versions

| Version | Repo Install | Setup Execution | vLLM Command |
|---------|--------------|-----------------|--------------|
| v0.0.70 | Manual | Bash script (hidden layers) | Hidden |
| v0.0.71 | Manual | Manual git commands | Explicit |
| v0.0.73 | Manual | Bash script (explicit) | Explicit |
| **v0.0.74** | **Automatic** | **Direct Python (automatic)** | **Explicit** |

## Migration Path

**From v0.0.73 to v0.0.74:**

Old workflow (2 commands):
```bash
python3 tt-jukebox.py --model llama --setup
bash ~/tt-scratchpad/setup-scripts/setup_llama_3_1_8b_instruct.sh
```

New workflow (1 command):
```bash
python3 tt-jukebox.py --model llama
# Auto-executes setup, shows vLLM command
```

**Breaking changes:**
- `--setup` flag removed (setup is automatic now)
- No more bash scripts generated in `~/tt-scratchpad/setup-scripts/`

## Future Considerations

**Potential separate repo:**
User mentioned: "Maybe we should make this project a more dedicated separate repo"

**Considerations:**
- Jukebox is growing beyond just VSCode extension support
- Could be standalone CLI tool for Tenstorrent environment management
- Would benefit from:
  - Dedicated documentation
  - Separate versioning
  - Installation via pip/homebrew
  - Integration with multiple tools (not just VSCode)

**For now:** Keep in extension, prove the concept works well.

## User Experience Impact

**Before v0.0.74:**
- Install tt-metal and vLLM manually
- Run jukebox to generate setup script
- Run bash script
- Hope everything works
- Copy vLLM command

**After v0.0.74:**
- Run jukebox (installs repos if needed)
- Confirm setup
- Wait for automatic execution
- Copy vLLM command

**Time saved:** 5-10 minutes of manual git/download commands

**Error reduction:** No more "forgot to checkout commit" or "wrong repo location" issues

**Confidence:** Everything is automated and validated before showing the final command
